# Code Contest
A repository for the course project of RL
---
## Project Description
This project is a reinforcement learning based code contest system. The goal is to design a system that can automatically generate code for a given problem and evaluate the performance of the generated code. The system will be implemented with Python.

## Project Structure
The project is organized into several folders:

- `code`: Contains code files generated for model evaluation (for future use).
- `config`: Contains configuration files for the model (for future use).
- `data`: Contains the data generated by `process.py` for training and testing the model.
- `raw_data`: Contains the raw data.
- `results`: Contains the results for platform submission.
- `scripts`: Contains utility functions used for data preprocessing, model training, and evaluation.
- `tmp`: Contains temporary cpp/exe files (cleaned up after evaluation).

## Usage
To use the project, you need to install the required packages and run the following commands:

1. Install the required packages:
   
   ```bash
   pip install -r requirements.txt
   ```
2. Preprocess the raw data (only when data folder is empty or needs modification):

   Put your raw data in the `raw_data` folder.
   ```
   ---raw_data
    |--- 1_ShellGame
    | |--- data
    | | |--- 1.in
    | | |--- 1.out
    | | |--- ...
    | |--- 题面.md
    | 
    |--- 2_SleepCowHerding
    | |--- data
    | | |--- 1.in
    | | |--- 1.out
    | | |--- ...
    | |--- 题面.md
    | 
    ...
   ```
   Then run:
    ```bash
    python scripts/process_data.py
    ```
   This will generate the data in the `data` folder.

3. Generate code:

   ```bash
   python scripts/llm_coder.py --model_name "glm"
   ```

4. Run code:

   Utilize `scripts/run_code.py` to run the generated code.

5. Submit and evaluate code:

   ```bash
   python scripts/frontend.py <Competition ID>
   ```

   The competition ID can be found on the platform.

   Note: This API provide by the organizers is unbelivably slow.

## Evaluation